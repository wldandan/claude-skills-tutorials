# CPU 异常检测功能 - 验收测试用例

## 文档信息
- **功能编号**: Feature 01
- **功能名称**: CPU 异常检测与分析
- **版本**: v1.0
- **创建日期**: 2026-01-16
- **测试负责人**: aiops-cli-tester

---

## 测试环境要求

### 硬件环境
- **CPU**: 多核处理器（至少 4 核）
- **内存**: 至少 8GB RAM
- **磁盘**: 至少 10GB 可用空间

### 软件环境
- **操作系统**: Linux (内核 >= 3.10)
  - 推荐版本: Ubuntu 18.04+ / CentOS 7+
- **Python**: Python 3.8+
- **依赖库**: 见规格文档依赖项章节

### 测试工具
- **测试框架**: pytest >= 7.0.0
- **覆盖率工具**: pytest-cov >= 4.0.0
- **性能测试**: pytest-benchmark >= 4.0.0
- **Mock 工具**: pytest-mock >= 3.10.0
- **故障注入**: 自定义 chaos 模块

---

## AC 1: 数据采集准确性

### AT-1.1: CPU 使用率采集准确性
- **用例编号**: AT-1.1
- **测试场景**: 验证 CPU 使用率采集与 /proc/stat 计算结果一致
- **优先级**: P0
- **前置条件**:
  - Linux 系统正常运行
  - aiops-cli 已安装
  - 系统 CPU 负载在正常范围内（20-80%）
- **测试步骤**:
  1. 启动系统 CPU 监控: `aiops collect cpu --duration 60s --output /tmp/cpu_test.json`
  2. 同时独立读取 /proc/stat 文件，手动计算 CPU 使用率
  3. 等待采集完成
  4. 对比采集结果与手动计算结果
- **预期结果**:
  - 采集的 CPU 使用率与手动计算结果误差 < 1%
  - User/System/Idle/IO Wait 时间占比总和为 100%
- **验证方法**:
  - 自动化比对脚本验证
  - 误差计算公式: `|采集值 - 手动计算值| < 1%`
- **清理步骤**:
  - 删除 /tmp/cpu_test.json

### AT-1.2: 多核 CPU 数据采集
- **用例编号**: AT-1.2
- **测试场景**: 验证每个逻辑核心的数据准确采集
- **优先级**: P0
- **前置条件**:
  - 系统为多核 CPU（至少 4 核）
  - aiops-cli 已安装
- **测试步骤**:
  1. 执行: `aiops collect cpu --per-core --duration 10s --output /tmp/per_core.json`
  2. 读取 /proc/stat 确认逻辑核心数量
  3. 解析输出文件，验证每个核心的数据
- **预期结果**:
  - 输出包含所有逻辑核心的数据
  - 每个核心的数据格式正确
  - 核心编号与 /proc/stat 一致
- **验证方法**:
  - 检查输出文件中核心数量与系统一致
  - 验证每个核心的 CPU 时间字段完整

### AT-1.3: CPU 时间分量完整性
- **用例编号**: AT-1.3
- **测试场景**: 验证 CPU 时间各分量总和为 100%
- **优先级**: P0
- **前置条件**:
  - 系统正常运行
- **测试步骤**:
  1. 执行: `aiops collect cpu --duration 10s --output /tmp/cpu_components.json`
  2. 解析输出，提取 user、system、idle、iowait 等分量
  3. 计算各分量总和
- **预期结果**:
  - 所有分量占比总和 = 100% ± 0.1%（允许浮点误差）
  - 所有分量值非负
- **验证方法**:
  - 自动化脚本验证总和
  - 检查是否有负值

---

## AC 2: 异常检测准确性

### AT-2.1: 静态阈值检测准确性
- **用例编号**: AT-2.1
- **测试场景**: 使用静态阈值检测已知的 CPU 异常
- **优先级**: P0
- **前置条件**:
  - 已准备测试数据集（包含标注的异常时段）
  - 测试数据中包含 CPU > 90% 的异常时段
- **测试步骤**:
  1. 使用故障注入工具模拟 CPU 高负载: `python chaos/cpu/stress_generator.py --load 95 --duration 300`
  2. 同时执行检测: `aiops detect cpu --threshold 90 --time-range 10m --algorithm static`
  3. 记录检测结果
  4. 停止故障注入
- **预期结果**:
  - 检测到异常事件
  - 检测准确率 >= 95%
  - 检测延迟 <= 30 秒
- **验证方法**:
  - 对比检测结果与实际注入时间
  - 计算准确率和延迟

### AT-2.2: 动态基线检测准确性
- **用例编号**: AT-2.2
- **测试场景**: 使用动态基线检测周期性异常
- **优先级**: P1
- **前置条件**:
  - 已有 7 天历史数据作为基线
  - 系统存在周期性负载模式
- **测试步骤**:
  1. 模拟周期性 CPU 负载（每天固定时间高峰）
  2. 在非高峰时段注入异常负载
  3. 执行: `aiops detect cpu --algorithm dynamic --baseline-days 7`
  4. 分析检测结果
- **预期结果**:
  - 准确区分正常高峰和异常负载
  - 误报率 <= 5%
  - 检测到真实的异常时段
- **验证方法**:
  - 人工审核检测结果
  - 统计误报和漏报数量

### AT-2.3: 机器学习算法检测准确性
- **用例编号**: AT-2.3
- **测试场景**: 使用 Isolation Forest 算法检测复杂异常
- **优先级**: P1
- **前置条件**:
  - 已准备标注测试集（包含正常和异常样本）
  - 已训练或配置好 ML 模型
- **测试步骤**:
  1. 加载标注测试数据
  2. 执行: `aiops detect cpu --algorithm isolation-forest --data-file test_dataset.csv`
  3. 对比预测结果与标注
  4. 计算精确率、召回率、F1 分数
- **预期结果**:
  - 准确率 >= 95%
  - 召回率 >= 90%
  - F1 分数 >= 0.92
- **验证方法**:
  - 使用 sklearn.metrics 计算指标
  - 生成混淆矩阵

### AT-2.4: 检测延迟测试
- **用例编号**: AT-2.4
- **测试场景**: 验证异常检测的实时性
- **优先级**: P0
- **前置条件**:
  - 系统处于正常负载状态
- **测试步骤**:
  1. 启动实时监控: `aiops monitor cpu --interval 1 --alert-threshold 90`
  2. 记录启动时间 T0
  3. 在 T0+10s 时注入 CPU 高负载
  4. 记录告警触发时间 T1
  5. 计算检测延迟 = T1 - (T0+10s)
  6. 重复 10 次取平均值
- **预期结果**:
  - 平均检测延迟 <= 30 秒
  - 最大延迟 <= 60 秒
- **验证方法**:
  - 自动化脚本测量延迟
  - 统计分析

---

## AC 3: 进程分析完整性

### AT-3.1: 进程 CPU 使用率准确性
- **用例编号**: AT-3.1
- **测试场景**: 验证指定进程的 CPU 使用率测量准确性
- **优先级**: P1
- **前置条件**:
  - 测试进程正在运行
  - 进程 CPU 使用率稳定在 10-50%
- **测试步骤**:
  1. 启动测试进程（CPU 密集型）: `stress --cpu 1 --timeout 60s &`
  2. 获取进程 PID: `$!`
  3. 执行: `aiops analyze cpu --pid <PID> --duration 10s`
  4. 同时读取 /proc/<PID>/stat 手动计算
  5. 对比结果
- **预期结果**:
  - 进程 CPU 使用率误差 < 2%
  - 显示进程名、PID、用户、CPU% 等信息
- **验证方法**:
  - 自动化比对
  - 误差计算

### AT-3.2: 线程级别分析
- **用例编号**: AT-3.2
- **测试场景**: 验证进程的线程级 CPU 分布分析
- **优先级**: P1
- **前置条件**:
  - 多线程进程正在运行
  - 进程至少有 4 个线程
- **测试步骤**:
  1. 启动多线程测试程序
  2. 执行: `aiops analyze cpu --pid <PID> --threads --detail`
  3. 读取 /proc/<PID>/task/ 验证线程数量
  4. 对比每个线程的 CPU 使用率
- **预期结果**:
  - 列出所有线程（TID）
  - 每个线程的 CPU 使用率准确
  - 显示线程名或线程号
- **验证方法**:
  - 验证线程数量一致
  - 抽样验证线程 CPU 准确性

### AT-3.3: CPU 亲和性显示
- **用例编号**: AT-3.3
- **测试场景**: 验证进程/线程的 CPU 亲和性显示
- **优先级**: P2
- **前置条件**:
  - 进程设置了 CPU 亲和性
- **测试步骤**:
  1. 启动测试进程并绑定到特定 CPU 核心
  2. 执行: `aiops analyze cpu --pid <PID> --affinity`
  3. 读取 /proc/<PID>/status 的 Cpus_allowed 验证
- **预期结果**:
  - 显示进程绑定的 CPU 核心列表
  - 与 /proc/<PID>/status 一致
- **验证方法**:
  - 对比输出与系统文件

### AT-3.4: Top N 进程识别
- **用例编号**: AT-3.4
- **测试场景**: 验证 Top 进程列表的正确性
- **优先级**: P0
- **前置条件**:
  - 系统有多个进程运行
  - 存在明显的 CPU 占用差异
- **测试步骤**:
  1. 启动多个测试进程，不同 CPU 占用
  2. 执行: `aiops analyze cpu --top 10 --sort cpu`
  3. 使用 ps 命令验证排序
- **预期结果**:
  - Top 10 进程按 CPU 使用率降序排列
  - 与 ps aux --sort=-%cpu | head -11 结果一致
- **验证方法**:
  - 交叉验证
  - 排序正确性检查

---

## AC 4: 性能与资源占用

### AT-4.1: 工具自身 CPU 占用
- **用例编号**: AT-4.1
- **测试场景**: 验证监控工具自身的 CPU 占用
- **优先级**: P0
- **前置条件**:
  - 系统负载正常
- **测试步骤**:
  1. 启动监控: `aiops monitor cpu --daemon --interval 1 --log-file /tmp/monitor.log`
  2. 获取监控进程 PID
  3. 使用 pidstat 或 top 持续监控该进程的 CPU 占用
  4. 记录 5 分钟内的 CPU 使用率
  5. 分析数据
- **预期结果**:
  - 平均 CPU 占用 < 2%（单核）
  - 峰值 CPU 占用 < 5%
- **验证方法**:
  - pidstat -p <PID> 1 300
  - 统计分析

### AT-4.2: 内存占用测试
- **用例编号**: AT-4.2
- **测试场景**: 验证工具的内存占用
- **优先级**: P0
- **前置条件**:
  - 监控工具正在运行
- **测试步骤**:
  1. 启动监控并记录初始内存: `aiops monitor cpu --daemon`
  2. 每 10 秒记录一次内存占用（持续 10 分钟）
  3. 分析内存增长趋势
- **预期结果**:
  - 初始内存占用 < 50MB
  - 稳定运行后内存 < 100MB
  - 无明显内存泄漏（10 分钟增长 < 10MB）
- **验证方法**:
  - /proc/<PID>/status 或 ps aux
  - 内存增长曲线分析

### AT-4.3: 磁盘 I/O 占用
- **用例编号**: AT-4.3
- **测试场景**: 验证数据持久化的磁盘写入量
- **优先级**: P1
- **前置条件**:
  - 配置了数据持久化
- **测试步骤**:
  1. 清空数据目录
  2. 启动监控并记录时间
  3. 运行 1 小时
  4. 统计数据目录大小和文件数量
- **预期结果**:
  - 磁盘写入 < 10MB/hour
  - 写入速率稳定，无突发大量写入
- **验证方法**:
  - du -sh 数据目录
  - iotop 监控

---

## AC 5: 命令行输出格式

### AT-5.1: JSON 输出格式验证
- **用例编号**: AT-5.1
- **测试场景**: 验证 JSON 输出格式符合 Schema
- **优先级**: P0
- **前置条件**:
  - 已定义 JSON Schema 文件
- **测试步骤**:
  1. 执行命令: `aiops detect cpu --output json --output-file /tmp/result.json`
  2. 使用 JSON Schema 验证工具检查
  3. 验证必需字段存在
  4. 验证数据类型正确
- **预期结果**:
  - JSON 格式正确，可解析
  - 所有必需字段存在
  - 数据类型符合 Schema
- **验证方法**:
  - jsonschema 库验证
  - jq 工具解析测试

### AT-5.2: YAML 输出格式验证
- **用例编号**: AT-5.2
- **测试场景**: 验证 YAML 输出格式
- **优先级**: P1
- **前置条件**:
  - 已定义 YAML 格式规范
- **测试步骤**:
  1. 执行: `aiops detect cpu --output yaml --output-file /tmp/result.yaml`
  2. 使用 YAML 解析器验证
  3. 检查字段完整性
- **预期结果**:
  - YAML 格式正确
  - 可正确解析
  - 字段与 JSON 一致
- **验证方法**:
  - Python yaml 库解析
  - 人工审查

### AT-5.3: 表格输出格式验证
- **用例编号**: AT-5.3
- **测试场景**: 验证表格输出的可读性和对齐
- **优先级**: P0
- **前置条件**:
  - 终端支持 UTF-8 和宽字符
- **测试步骤**:
  1. 执行: `aiops detect cpu --output table`
  2. 截图或保存终端输出
  3. 检查表格对齐
  4. 检查异常高亮显示
- **预期结果**:
  - 表格对齐美观
  - 列宽自适应内容
  - 异常数据用颜色高亮（红色/黄色）
  - 边框线条完整
- **验证方法**:
  - 人工审查
  - 自动检查终端转义序列

### AT-5.4: 多格式一致性
- **用例编号**: AT-5.4
- **测试场景**: 验证不同输出格式包含相同数据
- **优先级**: P1
- **前置条件**:
  - 系统有异常数据
- **测试步骤**:
  1. 分别生成 JSON、YAML、Table 输出
  2. 解析各格式数据
  3. 对比关键字段值
- **预期结果**:
  - 三种格式的核心数据一致
  - 异常事件数量相同
  - 时间戳、数值字段一致
- **验证方法**:
  - 自动化对比脚本
  - 差异检测

---

## AC 6: 历史数据查询

### AT-6.1: 时间范围查询
- **用例编号**: AT-6.1
- **测试场景**: 查询指定时间范围的数据
- **优先级**: P0
- **前置条件**:
  - 已采集 30 天的历史数据
  - 数据存储在数据库或文件中
- **测试步骤**:
  1. 执行: `aiops query cpu --start "2024-01-01" --end "2024-01-07" --output json`
  2. 验证返回的数据点时间戳
  3. 统计数据点数量
- **预期结果**:
  - 返回的时间戳在指定范围内
  - 数据点数量符合预期（7天 * 24小时 * 3600秒 / 采样间隔）
- **验证方法**:
  - 解析 JSON，检查时间范围
  - 数量验证

### AT-6.2: 查询性能测试
- **用例编号**: AT-6.2
- **测试场景**: 验证查询响应时间
- **优先级**: P0
- **前置条件**:
  - 有 30 天历史数据
- **测试步骤**:
  1. 执行查询并计时: `time aiops query cpu --start "2024-01-01" --end "2024-01-07"`
  2. 重复 5 次取平均
  3. 测试不同时间范围（1天、7天、30天）
- **预期结果**:
  - 7 天数据查询 < 3 秒
  - 1 天数据查询 < 1 秒
  - 30 天数据查询 < 10 秒
- **验证方法**:
  - time 命令测量
  - 性能基准测试

### AT-6.3: 聚合查询功能
- **用例编号**: AT-6.3
- **测试场景**: 验证数据聚合功能（avg/max/min/stddev）
- **优先级**: P1
- **前置条件**:
  - 有历史数据
- **测试步骤**:
  1. 执行聚合查询:
     ```bash
     aiops query cpu --start "2024-01-01" --end "2024-01-07" --aggregate avg
     aiops query cpu --start "2024-01-01" --end "2024-01-07" --aggregate max
     ```
  2. 手动计算原始数据的聚合值
  3. 对比结果
- **预期结果**:
  - 聚合值计算正确
  - 返回聚合后的时间序列
  - 支持 avg、max、min、stddev
- **验证方法**:
  - 与 Python pandas 计算结果对比
  - 数值精度检查

### AT-6.4: 分组查询
- **用例编号**: AT-6.4
- **测试场景**: 按时间粒度分组查询
- **优先级**: P2
- **前置条件**:
  - 有详细历史数据
- **测试步骤**:
  1. 执行分组查询:
     ```bash
     aiops query cpu --start "2024-01-01" --end "2024-01-07" --group-by hour
     ```
  2. 验证每组的时间范围
- **预期结果**:
  - 数据按小时分组
  - 每组包含该小时的聚合值
  - 时间边界正确
- **验证方法**:
  - 时间戳检查
  - 分组数量验证

---

## AC 7: eBPF 集成

### AT-7.1: eBPF 环境检测
- **用例编号**: AT-7.1
- **测试场景**: 验证 eBPF 功能可用性检测
- **优先级**: P1
- **前置条件**:
  - 系统内核 >= 4.1
- **测试步骤**:
  1. 执行: `aiops check-requirements --ebpf`
  2. 验证检测结果
- **预期结果**:
  - 正确报告 eBPF 支持状态
  - 如果不支持，给出明确原因
- **验证方法**:
  - 手动检查内核版本
  - 检查 BCC 工具是否可用

### AT-7.2: On-CPU 火焰图生成
- **用例编号**: AT-7.2
- **测试场景**: 验证 eBPF 火焰图生成
- **优先级**: P1
- **前置条件**:
  - eBPF 环境可用
  - 目标进程正在运行
- **测试步骤**:
  1. 启动 CPU 密集型测试进程
  2. 执行: `aiops profile cpu --pid <PID> --duration 30s --output /tmp/flamegraph.svg`
  3. 验证生成的 SVG 文件
- **预期结果**:
  - 生成有效的 SVG 文件
  - 火焰图显示调用栈
  - 热点函数突出显示
  - 文件可被浏览器打开
- **验证方法**:
  - 文件存在性检查
  - SVG 有效性验证
  - 浏览器打开测试

### AT-7.3: eBPF 采集开销
- **用例编号**: AT-7.3
- **测试场景**: 验证 eBPF 采集的性能开销
- **优先级**: P0
- **前置条件**:
  - eBPF 环境可用
- **测试步骤**:
  1. 记录系统初始 CPU 使用率
  2. 启动 eBPF 采集: `aiops profile cpu --duration 60s`
  3. 监控采集期间 CPU 使用率
  4. 计算额外开销
- **预期结果**:
  - 采集开销 < 5% CPU
  - 系统响应无明显延迟
- **验证方法**:
  - 对比启用/不启用的 CPU 占用
  - 性能测试工具验证

---

## AC 8: 交叉引用

### AT-8.1: 内存指标关联
- **用例编号**: AT-8.1
- **测试场景**: 验证 CPU 异常时关联内存指标
- **优先级**: P2
- **前置条件**:
  - 已采集内存指标
  - 存在 CPU 异常事件
- **测试步骤**:
  1. 检测到 CPU 异常
  2. 查看检测报告中的交叉引用部分
  3. 验证内存数据关联
- **预期结果**:
  - 显示同时段的内存使用率
  - 如果内存也异常，给出提示
  - 提供内存详细分析链接
- **验证方法**:
  - 人工审查报告
  - 时间戳对齐验证

### AT-8.2: 日志事件关联
- **用例编号**: AT-8.2
- **测试场景**: 验证异常时段的日志关联
- **优先级**: P2
- **前置条件**:
  - 已配置日志采集
  - 有日志数据
- **测试步骤**:
  1. 触发 CPU 异常
  2. 查看检测报告
  3. 点击日志链接或执行推荐命令
- **预期结果**:
  - 提供异常时段的日志摘要
  - 推荐日志查询命令
  - 日志时间范围对齐
- **验证方法**:
  - 人工测试
  - 日志时间戳验证

### AT-8.3: 告警历史关联
- **用例编号**: AT-8.3
- **测试场景**: 验证历史告警的关联
- **优先级**: P2
- **前置条件**:
  - 有历史告警数据
- **测试步骤**:
  1. 查询当前 CPU 异常
  2. 检查报告中的历史告警部分
  3. 验证关联的准确性
- **预期结果**:
  - 显示相关的历史告警
  - 标注相似告警模式
  - 提供历史事件对比
- **验证方法**:
  - 数据查询验证
  - 相关性评分

---

## 回归测试集

### RT-1: 基础功能回归
- **用例编号**: RT-1
- **测试场景**: 验证基础功能在版本更新后正常工作
- **执行频率**: 每次构建
- **包含用例**: AT-1.1, AT-1.2, AT-2.1, AT-5.1

### RT-2: 性能回归
- **用例编号**: RT-2
- **测试场景**: 验证性能无退化
- **执行频率**: 每周
- **包含用例**: AT-4.1, AT-4.2, AT-6.2

### RT-3: 端到端场景回归
- **用例编号**: RT-3
- **测试场景**: 完整使用流程验证
- **执行频率**: 每次发布
- **测试场景**:
  1. 采集数据 10 分钟
  2. 检测异常
  3. 分析进程
  4. 生成报告

---

## 测试数据管理

### 测试数据集
- **正常负载数据**: 7 天的正常 CPU 使用数据
- **异常数据集**: 包含各种异常类型的标注数据
  - 高 CPU 使用率
  - 单核过载
  - 进程异常
  - 周期性尖峰
- **边界数据**: 极端值测试数据
  - CPU 0% （空闲）
  - CPU 100% （满载）
  - 大量进程

### 数据生成工具
- `tests/mocks/generate_test_data.py`: 生成合成测试数据
- `chaos/cpu/stress_generator.py`: 生成真实异常负载

---

## 测试执行计划

### P0 用例执行
- **执行时机**: 每次 code review 通过后
- **覆盖率要求**: 100%
- **失败处理**: 阻止合并

### P1 用例执行
- **执行时机**: 每日构建
- **覆盖率要求**: >= 95%
- **失败处理**: 创建 bug，可阻塞发布

### P2 用例执行
- **执行时机**: 每周构建
- **覆盖率要求**: >= 80%
- **失败处理**: 记录问题，排期修复

---

## 测试报告模板

### 测试摘要
- 测试执行日期
- 测试版本
- 测试环境
- 执行人

### 测试结果统计
- 总用例数
- 通过数
- 失败数
- 跳过数
- 通过率

### 失败用例详情
- 用例编号
- 失败原因
- 错误日志
- Bug 链接

### 覆盖率报告
- 代码覆盖率
- 分支覆盖率
- 功能覆盖率

---

## 附录

### A. 测试环境搭建指南
详见: `/test-plans/test-environment-setup.md`

### B. 故障注入工具使用指南
详见: `/chaos/README.md`

### C. 持续集成配置
详见: `/.github/workflows/test.yml`
