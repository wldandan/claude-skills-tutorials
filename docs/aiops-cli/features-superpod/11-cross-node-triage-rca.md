# 特性 11: 跨节点问题定界与根因分析

## 功能概述

提供跨节点的智能问题定界和根因分析能力,在超节点(SuperPod)环境中自动识别故障的传播路径、影响范围和根本原因。通过分析节点间的依赖关系、资源状态和事件传播,实现分布式环境下的快速问题定位和根因推断。

## 用户场景

**场景 1: 服务不可用,需要快速定位故障节点**
- 某个微服务在大规模集群中不可用
- 不清楚是单个节点问题还是集群级问题
- 使用 `aiops triage --service payment-api --unavailable` 快速定位故障节点

**场景 2: 级联故障分析**
- 一个节点故障导致多个服务受影响
- 需要分析故障如何在节点间传播
- 使用 `aiops trace-failure --start-node node-1 --impact-scope` 追踪故障传播路径

**场景 3: 跨节点性能问题定位**
- 应用响应时间增加,但不确定是哪个节点的问题
- 需要分析跨节点调用链和资源竞争
- 使用 `aiops cross-node-rca --latency --time-range 30m` 定位性能瓶颈节点

## 技术方案概要

### 拓扑发现与建模
- **节点拓扑发现**: 自动发现 Kubernetes 集群节点拓扑
- **服务依赖图**: 构建微服务间的调用依赖关系
- **资源依赖图**: 识别共享资源(存储、网络、数据库)
- **因果图构建**: 基于结构因果模型(SCM)构建跨节点因果图

### 故障传播分析
- **传播路径追踪**: 追踪故障如何在节点间传播
- **影响范围计算**: 计算故障影响的节点和服务数量
- **传播速度分析**: 分析故障传播的速度和模式
- **阻断点识别**: 识别可以阻断故障传播的关键节点

### 跨节点定界策略
- **故障节点定位**: 通过排除法定位故障起始节点
- **问题层次划分**: 区分节点级、集群级、区域级问题
- **资源状态关联**: 关联多节点资源状态(CPU、内存、网络)
- **时间线对齐**: 对齐多节点事件时间线,识别最早异常点

## 核心功能点

### 1. 服务健康度全景视图
```bash
# 查看服务在所有节点的健康状态
aiops triage --service payment-api --cluster-wide
```
- 显示服务在每个节点的健康状态
- 识别不健康或不可用的节点
- 标注节点间健康度差异
- 提供节点资源状态对比

### 2. 跨节点故障定位
```bash
# 快速定位故障节点
aiops locate-fault --service frontend-api --symptom "高延迟"
```
- 分析服务在所有节点的性能指标
- 识别性能异常的节点
- 计算节点间性能差异显著性
- 提供故障节点置信度评分

### 3. 故障传播路径追踪
```bash
# 追踪故障传播路径
aiops trace-failure --start-node node-3 --start-time 2024-01-15T14:15:00
```
- 追踪故障如何在节点间传播
- 构建传播路径图
- 识别传播的关键节点和时间
- 分析传播模式和触发条件

### 4. 影响范围分析
```bash
# 分析故障影响范围
aiops impact-scope --node node-5 --failure-type "网络不可达"
```
- 计算受影响的节点数量
- 识别受影响的服务和 Pod
- 评估影响严重程度(用户数、请求量)
- 生成影响范围报告

### 5. 跨节点根因分析
```bash
# 跨节点自动根因分析
aiops cross-node-rca --event 2024-01-15T14:15:00 --cluster superpod-1
```
- 聚合多节点指标和日志
- 识别跨节点共同模式
- 分析节点间因果关系
- 生成根因假设和验证步骤

### 6. 节点对比分析
```bash
# 对比健康节点和故障节点
aiops compare-nodes --healthy node-1,node-2 --faulty node-3
```
- 对比节点资源使用差异
- 对比节点配置差异
- 对比节点版本差异
- 识别关键差异点

### 7. 集群级事件关联
```bash
# 关联集群级事件
aiops cluster-events --time-range 1h --correlate
```
- 聚合所有节点的事件
- 识别跨节点事件模式
- 关联节点级和集群级事件
- 识别事件风暴和级联故障

### 8. 分布式故障定界
```bash
# 分布式环境故障定界
aiops distributed-triage --symptom "服务不可用" --scope cluster
```
- 区分单节点故障 vs 集群级故障
- 区分应用层 vs 基础设施层故障
- 区分暂时性故障 vs 永久性故障
- 提供定界结论和处置建议

## 验收标准 (Acceptance Criteria)

### AC 1: 节点拓扑发现准确性
- **Given**: Kubernetes 集群包含 50 个节点
- **When**: 执行 `aiops discover-topology --cluster superpod-1`
- **Then**:
  - 正确发现所有节点的网络拓扑
  - 节点发现率 >= 99%
  - 节点关系(连接、依赖)准确率 >= 95%
  - 发现延迟 < 30 秒

### AC 2: 故障节点定位准确性
- **Given**: 集群中某节点发生故障(CPU 100%)
- **When**: 执行 `aiops locate-fault --service api-gateway`
- **Then**:
  - 正确识别故障节点(准确率 >= 90%)
  - 误报率 <= 10%(健康节点误判为故障)
  - 定位延迟 < 60 秒
  - 提供故障置信度评分

### AC 3: 故障传播路径准确性
- **Given**: 已知故障传播路径(node-1 -> node-2 -> node-3)
- **When**: 执行 `aiops trace-failure --start-node node-1`
- **Then**:
  - 正确重建传播路径(准确率 >= 85%)
  - 传播时间估计误差 < 20%
  - 识别传播触发点准确率 >= 80%
  - 提供传播路径可视化

### AC 4: 影响范围计算准确性
- **Given**: 节点 node-5 故障导致 5 个服务不可用
- **When**: 执行 `aiops impact-scope --node node-5`
- **Then**:
  - 正确计算受影响节点数量(误差 <= 1)
  - 正确识别受影响服务(准确率 >= 90%)
  - 影响评估(用户数、请求量)误差 < 30%
  - 生成影响范围报告

### AC 5: 跨节点根因分析准确性
- **Given**: 跨节点故障场景(例如:数据库连接池耗尽导致多节点故障)
- **When**: 执行 `aiops cross-node-rca --event <timestamp>`
- **Then**:
  - 根因假设包含真实根因(Top 3 中)
  - 根因假设合理性和可解释性评分 >= 4/5
  - 提供的验证步骤可执行
  - 分析延迟 < 5 分钟

### AC 6: 性能与资源占用
- **Given**: 100 节点集群运行正常
- **When**: 执行 `aiops triage --service api-gateway --cluster-wide`
- **Then**:
  - 分析时间 < 60 秒
  - 内存占用 < 2GB
  - CPU 占用 < 30%(短时间)
  - 网络开销 < 10MB

### AC 7: 节点对比分析完整性
- **Given**: 健康节点和故障节点各一个
- **When**: 执行 `aiops compare-nodes --healthy node-1 --faulty node-2`
- **Then**:
  - 对比所有关键资源指标(CPU、内存、网络、磁盘)
  - 对比节点配置(OS 版本、内核参数、Kubernetes 版本)
  - 识别显著差异(统计显著性 P < 0.05)
  - 差异点按重要性排序

### AC 8: 集群级事件关联准确性
- **Given**: 集群中发生事件风暴(例如:同时重启多个 Pod)
- **When**: 执行 `aiops cluster-events --correlate`
- **Then**:
  - 正确关联相关事件(准确率 >= 85%)
  - 识别事件模式和触发条件
  - 区分因果事件和巧合事件
  - 提供事件时间线可视化

## 依赖项

### 系统依赖
- **Kubernetes**: Kubernetes 1.19+ (支持 kubelet、kubectl)
- **Python**: Python 3.8+
- **权限**: Kubernetes 集群访问权限(RBAC)
- **网络**: 节点间网络连通性

### Python 库依赖
```
kubernetes>=24.0.0      # Kubernetes API 客户端
pandas>=2.0.0           # 数据处理和分析
numpy>=1.24.0           # 数值计算
scipy>=1.10.0           # 统计分析
scikit-learn>=1.3.0     # 机器学习算法
networkx>=3.1           # 图分析(拓扑和传播)
click>=8.1.0            # CLI 框架
rich>=13.0.0            # 终端美化
pyyaml>=6.0             # 配置文件解析
```

### 可选依赖
```
prometheus-client       # Prometheus 集成
plotly>=5.14.0          # 交互式图表
graphviz                # 图可视化
```

### 数据源
- **Kubernetes API**: 节点、Pod、Service 信息
- **kubelet**: 节点级指标(CAdvisor)
- **Prometheus**: 集群级指标(如果部署)
- **分布式追踪系统**: Jaeger/Zipkin(可选)

## 优先级

**P0 (必须实现)**
- AC 1: 节点拓扑发现准确性
- AC 2: 故障节点定位准确性
- 核心功能点 1, 2

**P1 (首版本必备)**
- AC 6: 性能与资源占用
- AC 3: 故障传播路径准确性
- AC 7: 节点对比分析完整性
- 核心功能点 3, 6, 8

**P2 (后续版本优化)**
- AC 4: 影响范围计算准确性
- AC 5: 跨节点根因分析准确性
- AC 8: 集群级事件关联准确性
- 核心功能点 4, 5, 7

## 输出示例

### 服务健康度全景视图
```bash
$ aiops triage --service payment-api --cluster-wide

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ 跨节点服务健康度分析                              2024-01-15 14:30:25 ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ 服务: payment-api (namespace: production)                        │
│ 集群: superpod-1 (节点数: 50)                                    │
│ 分析时间: 2024-01-15 14:00:00 - 14:30:00                         │
├──────────────────────────────────────────────────────────────────┤
│ 📊 服务健康度概览:                                                │
│   总节点数: 50                                                   │
│   健康节点: 42 (84%) ✅                                          │
│   警告节点: 5 (10%) ⚠️                                          │
│   故障节点: 3 (6%) 🚨                                            │
│                                                                  │
├──────────────────────────────────────────────────────────────────┤
│ 🚨 故障节点详情:                                                  │
│                                                                  │
│   node-3 (故障) - 置信度: 98%                                    │
│     故障类型: Pod CrashLoopBackOff                              │
│     故障时间: 2024-01-15 14:15:23                               │
│     持续时间: 15 分钟 2 秒                                       │
│     症状: payment-api Pod 持续重启                               │
│     资源状态:                                                     │
│       CPU: 95% (基线: 45%) ⚠️                                   │
│       内存: 87% (基线: 52%) ⚠️                                  │
│       磁盘: 正常                                                │
│       网络: 正常                                                │
│     根因假设: 内存不足导致 OOM kill                              │
│     建议: aiops analyze memory --node node-3 --pod payment-api  │
│                                                                  │
│   node-17 (故障) - 置信度: 92%                                   │
│     故障类型: 网络不可达                                         │
│     故障时间: 2024-01-15 14:12:10                               │
│     持续时间: 18 分钟 15 秒                                      │
│     症状: 无法访问 payment-api 服务                              │
│     根因假设: 网络策略配置错误                                   │
│                                                                  │
│   node-28 (故障) - 置信度: 85%                                   │
│     故障类型: 高延迟                                             │
│     故障时间: 2024-01-15 14:10:05                               │
│     症状: API 响应时间 > 5s (基线: 200ms)                       │
│     根因假设: 磁盘 IO 瓶颈影响容器性能                           │
│                                                                  │
├──────────────────────────────────────────────────────────────────┤
│ ⚠️  警告节点:                                                     │
│   node-5, node-8, node-12, node-19, node-23                     │
│   警告类型: 资源使用率接近限制(内存 > 75%)                       │
│                                                                  │
├──────────────────────────────────────────────────────────────────┤
│ 🔍 故障模式分析:                                                  │
│   故障分布: 分散在 3 个不同节点                                  │
│   时间相关性: 故障发生在 5 分钟窗口内(14:10-14:15)              │
│   空间相关性: 故障节点分布在不同可用区                           │
│   共同特征: 所有故障节点内存使用率均 > 80%                      │
│                                                                  │
│   推测: 可能是集群级内存压力导致个别节点内存不足                  │
│                                                                  │
├──────────────────────────────────────────────────────────────────┤
│ 💡 下一步建议:                                                    │
│   1. 分析故障节点:                                                │
│      aiops analyze node --name node-3 --detail                  │
│   2. 追踪故障传播:                                                │
│      aiops trace-failure --start-node node-3                    │
│   3. 跨节点根因分析:                                              │
│      aiops cross-node-rca --event 2024-01-15T14:15:23           │
│   4. 检查集群资源:                                                │
│      aiops cluster-resources --usage --trend                    │
│                                                                  │
└──────────────────────────────────────────────────────────────────┘
```

### 故障传播路径追踪
```bash
$ aiops trace-failure --start-node node-1 --start-time 2024-01-15T14:15:00

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ 故障传播路径分析                                    2024-01-15 14:30:25 ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ 起始节点: node-1                                                │
│ 起始时间: 2024-01-15 14:15:00                                   │
│ 故障类型: 数据库连接池耗尽                                      │
├──────────────────────────────────────────────────────────────────┤
│ 📊 传播路径图:                                                    │
│                                                                  │
│   14:15:00  [🔴 起始] node-1                                    │
│               数据库连接池耗尽                                   │
│                    ↓ (直接依赖, 延迟: 30s)                      │
│   14:15:30  [🟠 受影响] node-2, node-3, node-4                 │
│               应用服务响应变慢                                   │
│                    ↓ (服务依赖, 延迟: 1m)                       │
│   14:16:30  [🟡 轻微影响] node-5, node-6, node-7, node-8      │
│               上游服务队列堆积                                   │
│                    ↓ (级联效应, 延迟: 2m)                       │
│   14:18:30  [🟢 边界] 影响停止传播                               │
│               熔断器生效,隔离故障                                │
│                                                                  │
├──────────────────────────────────────────────────────────────────┤
│ 📈 传播统计:                                                      │
│   总影响节点数: 8 (16%)                                          │
│   直接影响: 1 节点 (2%)                                          │
│   间接影响: 7 节点 (14%)                                         │
│   传播深度: 3 层                                                │
│   传播速度: 2.7 节点/分钟                                        │
│   传播持续时间: 3 分 30 秒                                       │
│   阻断时间: 14:18:30 (熔断器生效)                               │
│                                                                  │
├──────────────────────────────────────────────────────────────────┤
│ 🔍 传播模式分析:                                                  │
│   传播类型: 级联故障(Cascading Failure)                         │
│   传播机制: 服务依赖链                                          │
│   触发条件: 数据库连接池耗尽 → 应用超时 → 上游队列堆积           │
│   阻断机制: 熔断器(Circuit Breaker)                             │
│   传播特点:                                                     │
│     • 快速传播(3 分钟内影响 8 节点)                             │
│     • 依赖关系明确(应用 → 上游服务)                             │
│     • 自动阻断(熔断器生效)                                      │
│                                                                  │
├──────────────────────────────────────────────────────────────────┤
│ 🎯 关键节点识别:                                                  │
│   node-1: 故障起始点, 根因节点                                   │
│     角色: 数据库客户端                                          │
│     问题: 连接池配置不当                                        │
│                                                                  │
│   node-2, node-3, node-4: 关键传播节点                          │
│     角色: 应用服务层                                            │
│     问题: 直接依赖数据库,无熔断保护                             │
│                                                                  │
│   node-5: 阻断节点                                              │
│     角色: API 网关                                              │
│     机制: 熔断器生效,阻断传播                                   │
│                                                                  │
├──────────────────────────────────────────────────────────────────┤
│ 💡 防护建议:                                                      │
│   1. 根因节点(node-1):                                           │
│      • 增加数据库连接池大小                                      │
│      • 配置连接池监控和告警                                      │
│                                                                  │
│   2. 关键传播节点(node-2,3,4):                                   │
│      • 增加熔断器配置                                            │
│      • 增加重试和退避策略                                        │
│                                                                  │
│   3. 阻断节点(node-5):                                           │
│      • 保持熔断器配置                                           │
│      • 增加降级策略                                              │
│                                                                  │
│   4. 集群级:                                                      │
│      • 配置资源隔离(QoS)                                        │
│      • 增加集群级监控和自动扩容                                  │
│                                                                  │
└──────────────────────────────────────────────────────────────────┘
```

### 跨节点根因分析
```bash
$ aiops cross-node-rca --event 2024-01-15T14:15:00 --cluster superpod-1

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ 跨节点根因分析                                      2024-01-15 14:30:25 ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ 事件时间: 2024-01-15 14:15:00                                  │
│ 集群: superpod-1 (50 节点)                                      │
│ 受影响节点: 8 (16%)                                             │
│ 分析时间范围: 14:00:00 - 14:30:00                              │
├──────────────────────────────────────────────────────────────────┤
│ 🔍 多节点共同模式分析:                                           │
│                                                                  │
│   共同症状:                                                      │
│     • 8 个节点的 payment-api 服务响应变慢                        │
│     • 6 个节点出现数据库连接超时                                 │
│     • 3 个节点 Pod 重启                                         │
│                                                                  │
│   共同指标异常:                                                  │
│     • 数据库连接数: 200/200 (100% 使用率) 🚨                   │
│     • 应用响应时间: 5.2s (基线: 200ms, +2500%) 🚨              │
│     • 内存使用率: 平均 78% (基线: 52%) ⚠️                      │
│                                                                  │
│   共同日志模式:                                                  │
│     • "Connection pool exhausted" (6 节点)                     │
│     • "Timeout waiting for connection" (8 节点)                │
│     • "OOM Killer" (3 节点)                                    │
│                                                                  │
├──────────────────────────────────────────────────────────────────┤
│ 🎯 根因假设 (按置信度排序):                                       │
│                                                                  │
│   假设 #1: 数据库连接池配置不当 (置信度: 94%)                   │
│                                                                  │
│     证据:                                                        │
│       ✅ 所有受影响节点都报告连接池耗尽                          │
│       ✅ 连接数达到上限(200/200)                                │
│       ✅ 错误日志模式高度一致                                   │
│       ✅ 时间线吻合(连接池耗尽 → 应用超时 → Pod 重启)           │
│                                                                  │
│     因果关系:                                                    │
│       连接池配置不当(上限太低)                                   │
│         → 高并发时连接池耗尽                                     │
│         → 应用等待连接超时                                       │
│         → 响应时间增加 → 内存堆积                                │
│         → OOM kill → Pod 重启                                   │
│                                                                  │
│     验证步骤:                                                    │
│       1. 检查数据库连接池配置:                                   │
│          kubectl get configmap db-config -o yaml               │
│       2. 查看数据库当前连接数:                                   │
│          aiops analyze database --connections                  │
│       3. 分析历史连接数趋势:                                      │
│          aiops trend database-connections --period 7d          │
│                                                                  │
│     修复建议:                                                    │
│       • 增加连接池上限: 200 → 500                               │
│       • 配置连接池监控告警                                       │
│       • 优化慢查询,减少连接占用时间                              │
│                                                                  │
│   ────────────────────────────────────────────────────────────  │
│                                                                  │
│   假设 #2: 数据库性能下降 (置信度: 65%)                         │
│                                                                  │
│     证据:                                                        │
│       ⚠️  部分节点报告慢查询                                     │
│       ⚠️  数据库 CPU 使用率增加                                  │
│       ❌ 不是所有节点都受影响                                    │
│                                                                  │
│     验证步骤:                                                    │
│       1. 分析数据库慢查询:                                       │
│          aiops analyze database --slow-queries                  │
│       2. 检查数据库锁等待:                                       │
│          aiops analyze database --locks                         │
│                                                                  │
│   ────────────────────────────────────────────────────────────  │
│                                                                  │
│   假设 #3: 突发流量 (置信度: 30%)                               │
│                                                                  │
│     证据:                                                        │
│       ⚠️  请求量增加 50%(需要验证)                              │
│       ❌ 流量增加不能解释连接池耗尽(除非远超历史峰值)           │
│                                                                  │
│     验证步骤:                                                    │
│       1. 分析请求量趋势:                                         │
│          aiops analyze traffic --service payment-api            │
│                                                                  │
├──────────────────────────────────────────────────────────────────┤
│ 📊 影响评估:                                                      │
│   受影响用户: 约 15,000 用户(30%)                                │
│   受影响请求: 约 300,000 请求(25%)                              │
│   业务损失: 约 $5,000 (估算)                                    │
│   MTTR: 18 分钟(从故障到恢复)                                   │
│                                                                  │
├──────────────────────────────────────────────────────────────────┤
│ 💡 下一步行动:                                                    │
│   立即:                                                          │
│     1. 增加数据库连接池上限                                      │
│     2. 重启受影响的 Pod                                         │
│                                                                  │
│   短期(24h):                                                     │
│     3. 配置连接池监控告警                                        │
│     4. 分析慢查询并优化                                          │
│                                                                  │
│   长期:                                                          │
│     5. 实施自动扩容策略                                          │
│     6. 配置熔断和降级机制                                        │
│     7. 进行容量规划                                              │
│                                                                  │
└──────────────────────────────────────────────────────────────────┘
```

## 后续演进方向

1. **自动故障隔离**: 基于故障传播分析,自动隔离故障节点
2. **预测性防护**: 基于历史模式,预测可能的级联故障并提前防护
3. **多集群支持**: 扩展到跨多个 Kubernetes 集群的问题定界
4. **自愈能力**: 自动修复常见故障模式(例如:重启 Pod、扩容)
