# å¤šæ¨¡æ€Agentï¼šè§†è§‰ã€è¯­éŸ³ä¸æ–‡æœ¬çš„èåˆ

> **æœ¬ç³»åˆ—ç®€ä»‹**ï¼šè¿™æ˜¯ä¸€å¥—ç³»ç»Ÿæ€§çš„AI AgentæŠ€æœ¯æ•™ç¨‹ï¼Œè¦†ç›–ä»åŸºç¡€æ¦‚å¿µåˆ°ç”Ÿäº§çº§åº”ç”¨çš„å®Œæ•´çŸ¥è¯†ä½“ç³»ã€‚æœ¬æ–‡æ˜¯ç³»åˆ—çš„ç¬¬9ç¯‡ã€‚

**ç³»åˆ—ç›®å½•**ï¼š
1. [AI Agentçš„æœ¬è´¨ï¼šä»è‡ªåŠ¨åŒ–åˆ°è‡ªä¸»æ™ºèƒ½](./article-01-agent-essence.md)
2. [Agentæ¶æ„è®¾è®¡ï¼šReActã€ReWOOä¸æ€ç»´é“¾](./article-02-agent-architecture.md)
3. [å·¥å…·è°ƒç”¨ï¼ˆFunction Callingï¼‰ï¼šAgentçš„æ‰‹å’Œè„š](./article-03-function-calling.md)
4. [MCPåè®®æ·±åº¦è§£æï¼šè¿æ¥AIä¸æ•°æ®æºçš„æ ‡å‡†åŒ–æ¡¥æ¢](./article-04-mcp-protocol.md)
5. [Workflowæ¶æ„ï¼šå¯è§†åŒ–Agentç¼–æ’å¹³å°](./article-05-workflow.md)
6. [Skillsç³»ç»Ÿï¼šClaude Codeçš„æ¨¡å—åŒ–èƒ½åŠ›å°è£…](./article-06-skills-system.md)
7. [è®°å¿†ç³»ç»Ÿï¼šè®©Agentæ‹¥æœ‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›](./article-07-memory-system.md)
8. [è§„åˆ’ä¸æ¨ç†ï¼šAgentå¦‚ä½•åˆ†è§£å¤æ‚ä»»åŠ¡](./article-08-planning-reasoning.md)
9. [å¤šæ¨¡æ€Agentï¼šè§†è§‰ã€è¯­éŸ³ä¸æ–‡æœ¬çš„èåˆ](./article-09-multimodal-agent.md)
10. [Agentè¯„ä¼°ä¸ä¼˜åŒ–ï¼šå¦‚ä½•è¡¡é‡Agentæ€§èƒ½](./article-10-agent-evaluation.md)
11. [Multi-Agentç³»ç»Ÿï¼šåä½œã€ç«äº‰ä¸æ¶Œç°](./article-11-multi-agent-systems.md)
12. [ç”Ÿäº§çº§Agentæ¶æ„ï¼šå¯é æ€§ã€å®‰å…¨æ€§ä¸å¯è§‚æµ‹æ€§](./article-12-production-agent.md)
13. [å®æˆ˜æ¡ˆä¾‹ï¼šæ„å»ºä¼ä¸šçº§AIåŠ©æ‰‹ï¼ˆå®Œæ•´é¡¹ç›®ï¼‰](./article-13-enterprise-ai-assistant.md)
14. [AI Agentçš„æœªæ¥ï¼šAGIä¹‹è·¯ä¸Šçš„å…³é”®ä¸€æ­¥](./article-14-future-of-agents.md)


---



> æœ¬æ–‡æ˜¯ã€ŠAI Agentç³»åˆ—æ•™ç¨‹ã€‹çš„ç¬¬9ç¯‡ï¼Œå°†æ·±å…¥æ¢è®¨å¤šæ¨¡æ€Agentçš„æ¶æ„ä¸å®ç°ï¼Œè¿™æ˜¯è®©Agentèƒ½å¤Ÿç†è§£å¹¶å¤„ç†è§†è§‰ã€å¬è§‰ç­‰å¤šç§æ¨¡æ€ä¿¡æ¯çš„å…³é”®æŠ€æœ¯ã€‚

## ä¸Šä¸€ç¯‡å›é¡¾

åœ¨ä¸Šä¸€ç¯‡æ–‡ç« ã€Šè§„åˆ’ä¸æ¨ç†ï¼šAgentå¦‚ä½•åˆ†è§£å¤æ‚ä»»åŠ¡ã€‹ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†Agentçš„è§„åˆ’ä¸æ¨ç†èƒ½åŠ›ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬çš„Agentä¸»è¦å¤„ç†**æ–‡æœ¬**æ¨¡æ€çš„ä¿¡æ¯ã€‚

ä½†çœŸå®ä¸–ç•Œæ˜¯å¤šæ¨¡æ€çš„ï¼š
- ç”¨æˆ·å¯èƒ½å‘é€ä¸€å¼ å›¾ç‰‡é—®"è¿™æ˜¯ä»€ä¹ˆï¼Ÿ"
- å¯èƒ½å‘ä¸€æ®µè¯­éŸ³è¯´"å¸®æˆ‘é¢„çº¦ä¼šè®®"
- å¯èƒ½åŒæ—¶æä¾›æ–‡æ¡£ã€å›¾è¡¨å’Œæ–‡å­—è¯´æ˜

**å¤šæ¨¡æ€Agent**å°±æ˜¯èƒ½å¤Ÿç†è§£ã€å¤„ç†å’Œç”Ÿæˆå¤šç§æ¨¡æ€ï¼ˆæ–‡æœ¬ã€å›¾åƒã€è¯­éŸ³ã€è§†é¢‘ç­‰ï¼‰ä¿¡æ¯çš„Agentç³»ç»Ÿã€‚

## å¼•è¨€ï¼šä¸ºä»€ä¹ˆéœ€è¦å¤šæ¨¡æ€ï¼Ÿ

### å•æ¨¡æ€çš„å±€é™

```
åœºæ™¯1ï¼šç”¨æˆ·ä¸Šä¼ ä¸€å¼ åŒ»ç–—Xå…‰ç‰‡
å•æ¨¡æ€æ–‡æœ¬Agentï¼š"æŠ±æ­‰ï¼Œæˆ‘åªèƒ½å¤„ç†æ–‡å­—ï¼Œæ— æ³•çœ‹å›¾ç‰‡"
å¤šæ¨¡æ€Agentï¼š"è¿™å¼ Xå…‰ç‰‡æ˜¾ç¤ºå³ä¸‹è‚ºæœ‰é˜´å½±ï¼Œå»ºè®®è¿›ä¸€æ­¥æ£€æŸ¥"

åœºæ™¯2ï¼šç”¨æˆ·å‘é€è¯­éŸ³æ¶ˆæ¯
å•æ¨¡æ€æ–‡æœ¬Agentï¼š[æ— æ³•ç†è§£]
å¤šæ¨¡æ€Agentï¼šè¯†åˆ«è¯­éŸ³ -> "å¥½çš„ï¼Œæˆ‘å¸®æ‚¨æŸ¥è¯¢æ˜å¤©åŒ—äº¬çš„å¤©æ°”"

åœºæ™¯3ï¼šç”¨æˆ·æä¾›åŒ…å«å›¾è¡¨çš„è´¢æŠ¥
å•æ¨¡æ€æ–‡æœ¬Agentï¼šåªèƒ½çœ‹æ–‡å­—æè¿°
å¤šæ¨¡æ€Agentï¼šåˆ†æå›¾è¡¨æ•°æ® + æ–‡å­—æè¿° -> ç»¼åˆåˆ†æ
```

### å¤šæ¨¡æ€çš„ä»·å€¼

1. **æ›´è‡ªç„¶çš„äº¤äº’**ï¼šåƒäººç±»ä¸€æ ·é€šè¿‡å¤šç§æ–¹å¼äº¤æµ
2. **æ›´ä¸°å¯Œçš„ä¿¡æ¯**ï¼šè§†è§‰ã€å¬è§‰åŒ…å«æ–‡æœ¬æ— æ³•è¡¨è¾¾çš„ä¿¡æ¯
3. **æ›´å¹¿æ³›çš„åº”ç”¨**ï¼šå›¾åƒåˆ†æã€è§†é¢‘ç†è§£ã€è¯­éŸ³äº¤äº’ç­‰
4. **æ›´å¥½çš„ä½“éªŒ**ï¼šç”¨æˆ·å¯ä»¥é€‰æ‹©æœ€èˆ’é€‚çš„äº¤äº’æ–¹å¼

## ä¸€ã€å¤šæ¨¡æ€åŸºç¡€æ¶æ„

### 1.1 å¤šæ¨¡æ€æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         å¤šæ¨¡æ€è¾“å…¥                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ æ–‡æœ¬ â”‚ â”‚ å›¾åƒ â”‚ â”‚ è¯­éŸ³ â”‚ â”‚ è§†é¢‘ â”‚   â”‚
â”‚  â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚        â”‚        â”‚        â”‚
      â–¼        â–¼        â–¼        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         æ¨¡æ€ç¼–ç å™¨                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ Text Encoder â”‚  Vision Encoder â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚  â”‚ Audio Encoder â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚          â”‚          â”‚
      â–¼          â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         å¤šæ¨¡æ€èåˆ                        â”‚
â”‚   - å¯¹é½ï¼ˆAlignmentï¼‰                     â”‚
â”‚   - æ³¨æ„åŠ›ï¼ˆAttentionï¼‰                   â”‚
â”‚   - è·¨æ¨¡æ€äº¤äº’                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LLMæ¨ç†å¼•æ“                       â”‚
â”‚   - å¤šæ¨¡æ€ç†è§£                            â”‚
â”‚   - è§„åˆ’ä¸å†³ç­–                            â”‚
â”‚   - å·¥å…·è°ƒç”¨                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         å¤šæ¨¡æ€è¾“å‡º                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ æ–‡æœ¬ â”‚ â”‚ å›¾åƒ â”‚ â”‚ è¯­éŸ³ â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ ¸å¿ƒç»„ä»¶

```python
from typing import List, Dict, Union, Optional
import base64
from PIL import Image
import io

class MultimodalInput:
    """å¤šæ¨¡æ€è¾“å…¥å®¹å™¨"""

    def __init__(self):
        self.text: Optional[str] = None
        self.images: List[Image.Image] = []
        self.audio: Optional[bytes] = None
        self.video: Optional[str] = None  # æ–‡ä»¶è·¯å¾„
        self.metadata: Dict = {}

    def add_text(self, text: str):
        """æ·»åŠ æ–‡æœ¬"""
        self.text = text
        return self

    def add_image(self, image: Union[str, Image.Image, bytes]):
        """æ·»åŠ å›¾åƒ"""
        if isinstance(image, str):
            # æ–‡ä»¶è·¯å¾„
            img = Image.open(image)
            self.images.append(img)
        elif isinstance(image, Image.Image):
            self.images.append(image)
        elif isinstance(image, bytes):
            img = Image.open(io.BytesIO(image))
            self.images.append(img)
        return self

    def add_audio(self, audio: bytes):
        """æ·»åŠ éŸ³é¢‘"""
        self.audio = audio
        return self

    def to_llm_format(self) -> List[Dict]:
        """è½¬æ¢ä¸ºLLM APIæ ¼å¼"""
        content = []

        # æ·»åŠ æ–‡æœ¬
        if self.text:
            content.append({
                "type": "text",
                "text": self.text
            })

        # æ·»åŠ å›¾åƒ
        for image in self.images:
            # è½¬æ¢ä¸ºbase64
            buffered = io.BytesIO()
            image.save(buffered, format="PNG")
            img_str = base64.b64encode(buffered.getvalue()).decode()

            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/png;base64,{img_str}"
                }
            })

        return [{"role": "user", "content": content}]

# ä½¿ç”¨ç¤ºä¾‹
input_data = MultimodalInput()
input_data.add_text("è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ")
input_data.add_image("path/to/image.jpg")

messages = input_data.to_llm_format()
```

## äºŒã€è§†è§‰ç†è§£èƒ½åŠ›

### 2.1 å›¾åƒæè¿°ä¸åˆ†æ

```python
class VisionAgent:
    """è§†è§‰Agent"""

    def __init__(self, model="gpt-4-vision-preview"):
        self.model = model
        self.client = openai.OpenAI()

    def describe_image(self, image_path: str, detail: str = "high") -> str:
        """æè¿°å›¾ç‰‡å†…å®¹"""
        # è¯»å–å›¾ç‰‡
        with open(image_path, "rb") as f:
            image_data = base64.b64encode(f.read()).decode()

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚"
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{image_data}",
                            "detail": detail  # "low", "high", "auto"
                        }
                    }
                ]
            }],
            max_tokens=500
        )

        return response.choices[0].message.content

    def extract_text_from_image(self, image_path: str) -> str:
        """OCRï¼šä»å›¾ç‰‡ä¸­æå–æ–‡å­—"""
        with open(image_path, "rb") as f:
            image_data = base64.b64encode(f.read()).decode()

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "è¯·æå–å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹ï¼Œä¿æŒåŸæœ‰æ ¼å¼ã€‚"
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{image_data}"
                        }
                    }
                ]
            }],
            max_tokens=1000
        )

        return response.choices[0].message.content

    def analyze_chart(self, image_path: str) -> Dict:
        """åˆ†æå›¾è¡¨"""
        with open(image_path, "rb") as f:
            image_data = base64.b64encode(f.read()).decode()

        prompt = """
        åˆ†æè¿™å¼ å›¾è¡¨ï¼Œæå–ä»¥ä¸‹ä¿¡æ¯ï¼š
        1. å›¾è¡¨ç±»å‹ï¼ˆæŸ±çŠ¶å›¾ã€æŠ˜çº¿å›¾ã€é¥¼å›¾ç­‰ï¼‰
        2. æ ‡é¢˜å’Œæ ‡ç­¾
        3. æ•°æ®è¶‹åŠ¿
        4. å…³é”®æ•°æ®ç‚¹
        5. ç»“è®ºå’Œæ´å¯Ÿ

        ä»¥JSONæ ¼å¼è¿”å›ã€‚
        """

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{image_data}"
                        }
                    }
                ]
            }],
            response_format={"type": "json_object"},
            max_tokens=1000
        )

        import json
        return json.loads(response.choices[0].message.content)

    def compare_images(self, image1_path: str, image2_path: str) -> str:
        """å¯¹æ¯”ä¸¤å¼ å›¾ç‰‡"""
        with open(image1_path, "rb") as f:
            img1_data = base64.b64encode(f.read()).decode()

        with open(image2_path, "rb") as f:
            img2_data = base64.b64encode(f.read()).decode()

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "å¯¹æ¯”è¿™ä¸¤å¼ å›¾ç‰‡ï¼ŒæŒ‡å‡ºç›¸åŒç‚¹å’Œä¸åŒç‚¹ã€‚"
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{img1_data}"
                        }
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{img2_data}"
                        }
                    }
                ]
            }],
            max_tokens=800
        )

        return response.choices[0].message.content
```

### 2.2 è§†è§‰é—®ç­”ï¼ˆVisual QAï¼‰

```python
class VisualQAAgent(VisionAgent):
    """è§†è§‰é—®ç­”Agent"""

    def answer_question(self, image_path: str, question: str) -> str:
        """å›ç­”å…³äºå›¾ç‰‡çš„é—®é¢˜"""
        with open(image_path, "rb") as f:
            image_data = base64.b64encode(f.read()).decode()

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": f"é—®é¢˜ï¼š{question}\n\nè¯·åŸºäºå›¾ç‰‡å†…å®¹å›ç­”é—®é¢˜ã€‚"
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{image_data}"
                        }
                    }
                ]
            }],
            max_tokens=500
        )

        return response.choices[0].message.content

    def multi_turn_conversation(self, image_path: str):
        """å¤šè½®å¯¹è¯"""
        conversation = []

        # ç¬¬ä¸€è½®ï¼šæè¿°å›¾ç‰‡
        with open(image_path, "rb") as f:
            image_data = base64.b64encode(f.read()).decode()

        conversation.append({
            "role": "user",
            "content": [
                {"type": "text", "text": "è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä»€ä¹ˆï¼Ÿ"},
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}
                }
            ]
        })

        response = self.client.chat.completions.create(
            model=self.model,
            messages=conversation
        )

        assistant_message = response.choices[0].message
        conversation.append(assistant_message)

        print("Assistant:", assistant_message.content)

        # åç»­è½®æ¬¡ï¼šç»§ç»­æé—®
        while True:
            user_input = input("\nYou: ")
            if user_input.lower() in ['exit', 'quit']:
                break

            conversation.append({
                "role": "user",
                "content": user_input
            })

            response = self.client.chat.completions.create(
                model=self.model,
                messages=conversation
            )

            assistant_message = response.choices[0].message
            conversation.append(assistant_message)

            print("Assistant:", assistant_message.content)
```

## ä¸‰ã€è¯­éŸ³äº¤äº’èƒ½åŠ›

### 3.1 è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰

```python
import whisper
from typing import Optional

class SpeechRecognitionAgent:
    """è¯­éŸ³è¯†åˆ«Agent"""

    def __init__(self, model_size: str = "base"):
        """
        model_size: tiny, base, small, medium, large
        """
        self.model = whisper.load_model(model_size)

    def transcribe(
        self,
        audio_path: str,
        language: str = "zh"
    ) -> str:
        """è½¬å½•è¯­éŸ³ä¸ºæ–‡å­—"""
        result = self.model.transcribe(
            audio_path,
            language=language,
            task="transcribe"
        )

        return result["text"]

    def transcribe_with_timestamps(
        self,
        audio_path: str,
        language: str = "zh"
    ) -> Dict:
        """å¸¦æ—¶é—´æˆ³çš„è½¬å½•"""
        result = self.model.transcribe(
            audio_path,
            language=language,
            word_timestamps=True
        )

        segments = []
        for segment in result["segments"]:
            segments.append({
                "start": segment["start"],
                "end": segment["end"],
                "text": segment["text"]
            })

        return {
            "text": result["text"],
            "language": result["language"],
            "segments": segments
        }

    def translate(self, audio_path: str, target_language: str = "en") -> str:
        """ç¿»è¯‘è¯­éŸ³"""
        result = self.model.transcribe(
            audio_path,
            task="translate"
        )

        return result["text"]

# ä½¿ç”¨ç¤ºä¾‹
asr_agent = SpeechRecognitionAgent(model_size="base")

# è½¬å½•ä¸­æ–‡è¯­éŸ³
text = asr_agent.transcribe("voice_message.m4a", language="zh")
print(f"è¯†åˆ«ç»“æœï¼š{text}")

# å¸¦æ—¶é—´æˆ³
detailed = asr_agent.transcribe_with_timestamps("voice_message.m4a")
print(f"è¯¦ç»†ç»“æœï¼š{detailed}")
```

### 3.2 è¯­éŸ³åˆæˆï¼ˆTTSï¼‰

```python
from openai import OpenAI
import pygame
import tempfile
import os

class TextToSpeechAgent:
    """æ–‡æœ¬è½¬è¯­éŸ³Agent"""

    def __init__(self):
        self.client = OpenAI()
        pygame.mixer.init()

    def synthesize(
        self,
        text: str,
        voice: str = "alloy",
        output_path: Optional[str] = None
    ) -> str:
        """
        voice: alloy, echo, fable, onyx, nova, shimmer
        """
        response = self.client.audio.speech.create(
            model="tts-1",
            voice=voice,
            input=text
        )

        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        if output_path is None:
            output_path = tempfile.mktemp(suffix=".mp3")

        response.stream_to_file(output_path)

        return output_path

    def speak(self, text: str, voice: str = "alloy"):
        """æ’­æ”¾è¯­éŸ³"""
        audio_file = self.synthesize(text, voice)

        # æ’­æ”¾
        pygame.mixer.music.load(audio_file)
        pygame.mixer.music.play()

        while pygame.mixer.music.get_busy():
            pygame.time.Clock().tick(10)

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(audio_file)

    def synthesize_with_emotion(
        self,
        text: str,
        emotion: str = "neutral"
    ) -> str:
        """å¸¦æƒ…æ„Ÿçš„è¯­éŸ³åˆæˆ"""
        # æ ¹æ®æƒ…æ„Ÿé€‰æ‹©å£°éŸ³
        emotion_voice_map = {
            "neutral": "alloy",
            "happy": "nova",
            "serious": "onyx",
            "gentle": "shimmer",
            "energetic": "echo"
        }

        voice = emotion_voice_map.get(emotion, "alloy")

        # è°ƒæ•´æ–‡æœ¬ï¼ˆæ·»åŠ æƒ…æ„Ÿæ ‡è®°ï¼‰
        emotion_prompts = {
            "neutral": text,
            "happy": f"{text}ï¼",
            "serious": text,
            "gentle": f"{text}ï½",
            "energetic": f"{text}ï¼"
        }

        adjusted_text = emotion_prompts.get(emotion, text)

        return self.synthesize(adjusted_text, voice)
```

### 3.3 è¯­éŸ³å¯¹è¯Agent

```python
class VoiceConversationAgent:
    """è¯­éŸ³å¯¹è¯Agent"""

    def __init__(self):
        self.asr = SpeechRecognitionAgent(model_size="base")
        self.tts = TextToSpeechAgent()
        self.llm_client = OpenAI()
        self.conversation_history = []

    def listen_and_respond(self, audio_path: str, voice: str = "alloy"):
        """å¬éŸ³é¢‘å¹¶å›å¤"""
        # 1. è¯­éŸ³è¯†åˆ«
        print("ğŸ¤ æ­£åœ¨è¯†åˆ«è¯­éŸ³...")
        user_text = self.asr.transcribe(audio_path)
        print(f"ğŸ‘¤ ç”¨æˆ·ï¼š{user_text}")

        # 2. LLMç”Ÿæˆå›å¤
        print("ğŸ¤” æ­£åœ¨æ€è€ƒ...")
        self.conversation_history.append({
            "role": "user",
            "content": user_text
        })

        response = self.llm_client.chat.completions.create(
            model="gpt-4",
            messages=self.conversation_history
        )

        assistant_text = response.choices[0].message.content
        print(f"ğŸ¤– åŠ©æ‰‹ï¼š{assistant_text}")

        self.conversation_history.append({
            "role": "assistant",
            "content": assistant_text
        })

        # 3. è¯­éŸ³åˆæˆå¹¶æ’­æ”¾
        print("ğŸ”Š æ­£åœ¨ç”Ÿæˆè¯­éŸ³...")
        self.tts.speak(assistant_text, voice=voice)

        return assistant_text

    def real_time_conversation(self):
        """å®æ—¶å¯¹è¯"""
        print("å¼€å§‹è¯­éŸ³å¯¹è¯ï¼ˆè¾“å…¥'quit'é€€å‡ºï¼‰")

        while True:
            # è¿™é‡Œåº”è¯¥ä½¿ç”¨å®æ—¶éŸ³é¢‘è¾“å…¥
            # ç®€åŒ–å¤„ç†ï¼šä½¿ç”¨é¢„å½•åˆ¶çš„éŸ³é¢‘æ–‡ä»¶
            audio_file = input("\nè¯·è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆæˆ–'quit'é€€å‡ºï¼‰: ")

            if audio_file.lower() == 'quit':
                break

            if not os.path.exists(audio_file):
                print("æ–‡ä»¶ä¸å­˜åœ¨ï¼")
                continue

            try:
                self.listen_and_respond(audio_file)
            except Exception as e:
                print(f"é”™è¯¯ï¼š{e}")
```

## å››ã€å¤šæ¨¡æ€èåˆAgent

### 4.1 ç»Ÿä¸€å¤šæ¨¡æ€Agent

```python
class UnifiedMultimodalAgent:
    """ç»Ÿä¸€å¤šæ¨¡æ€Agent"""

    def __init__(self):
        self.client = OpenAI()
        self.vision_agent = VisionAgent()
        self.speech_agent = SpeechRecognitionAgent()
        self.tts_agent = TextToSpeechAgent()
        self.memory = []

    def process(
        self,
        input_data: MultimodalInput,
        output_modality: str = "text"
    ) -> Union[str, bytes]:
        """
        å¤„ç†å¤šæ¨¡æ€è¾“å…¥

        output_modality: text, speech
        """
        # 1. è½¬æ¢ä¸ºLLMæ ¼å¼
        messages = input_data.to_llm_format()

        # 2. æ·»åŠ å†å²å¯¹è¯
        messages.extend(self.memory[-10:])  # ä¿ç•™æœ€è¿‘10è½®

        # 3. LLMæ¨ç†
        response = self.client.chat.completions.create(
            model="gpt-4-vision-preview",
            messages=messages,
            max_tokens=1000
        )

        assistant_message = response.choices[0].message.content

        # 4. ä¿å­˜åˆ°è®°å¿†
        self.memory.append(messages[0])
        self.memory.append({
            "role": "assistant",
            "content": assistant_message
        })

        # 5. æ ¹æ®éœ€æ±‚è¾“å‡ºä¸åŒæ¨¡æ€
        if output_modality == "text":
            return assistant_message
        elif output_modality == "speech":
            # è¯­éŸ³åˆæˆ
            audio_file = self.tts_agent.synthesize(assistant_message)
            with open(audio_file, "rb") as f:
                return f.read()
        else:
            return assistant_message

    def chat(self):
        """äº¤äº’å¼å¯¹è¯"""
        print("å¤šæ¨¡æ€Agentå¯åŠ¨ï¼ˆæ”¯æŒæ–‡æœ¬ã€å›¾ç‰‡ã€è¯­éŸ³ï¼‰")
        print("è¾“å…¥æ ¼å¼ï¼š")
        print("  - ç›´æ¥è¾“å…¥æ–‡å­—")
        print("  - 'image: <å›¾ç‰‡è·¯å¾„>'")
        print("  - 'voice: <éŸ³é¢‘è·¯å¾„>'")
        print("  è¾“å…¥'quit'é€€å‡º\n")

        while True:
            user_input = input("You: ")

            if user_input.lower() == 'quit':
                break

            # è§£æè¾“å…¥
            input_data = MultimodalInput()

            if user_input.startswith("image:"):
                image_path = user_input[6:].strip()
                input_data.add_image(image_path)
                input_data.add_text("è¯·æè¿°è¿™å¼ å›¾ç‰‡ã€‚")
            elif user_input.startswith("voice:"):
                audio_path = user_input[6:].strip()
                # è½¬å½•è¯­éŸ³
                text = self.speech_agent.transcribe(audio_path)
                print(f"[è¯†åˆ«åˆ°è¯­éŸ³ï¼š{text}]")
                input_data.add_text(text)
            else:
                input_data.add_text(user_input)

            # å¤„ç†
            try:
                response = self.process(input_data, output_modality="text")
                print(f"\nAssistant: {response}\n")
            except Exception as e:
                print(f"é”™è¯¯ï¼š{e}\n")
```

### 4.2 å¤šæ¨¡æ€å·¥å…·è°ƒç”¨

```python
class MultimodalToolAgent(UnifiedMultimodalAgent):
    """æ”¯æŒå¤šæ¨¡æ€å·¥å…·è°ƒç”¨çš„Agent"""

    def __init__(self):
        super().__init__()
        self.tools = self._init_tools()

    def _init_tools(self):
        """åˆå§‹åŒ–å·¥å…·"""
        return {
            "analyze_image": {
                "description": "åˆ†æå›¾ç‰‡å†…å®¹",
                "function": self._tool_analyze_image
            },
            "transcribe_audio": {
                "description": "è½¬å½•éŸ³é¢‘ä¸ºæ–‡å­—",
                "function": self._tool_transcribe_audio
            },
            "search": {
                "description": "æœç´¢ä¿¡æ¯",
                "function": lambda q: f"æœç´¢ç»“æœï¼š{q}"
            }
        }

    def _tool_analyze_image(self, image_path: str) -> str:
        """å·¥å…·ï¼šåˆ†æå›¾ç‰‡"""
        return self.vision_agent.describe_image(image_path)

    def _tool_transcribe_audio(self, audio_path: str) -> str:
        """å·¥å…·ï¼šè½¬å½•éŸ³é¢‘"""
        return self.speech_agent.transcribe(audio_path)

    def process_with_tools(
        self,
        input_data: MultimodalInput
    ) -> str:
        """å¸¦å·¥å…·è°ƒç”¨çš„å¤„ç†"""
        messages = input_data.to_llm_format()

        # æ·»åŠ å·¥å…·å®šä¹‰
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "analyze_image",
                    "description": "åˆ†æå›¾ç‰‡å†…å®¹",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "image_path": {
                                "type": "string",
                                "description": "å›¾ç‰‡æ–‡ä»¶è·¯å¾„"
                            }
                        },
                        "required": ["image_path"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "transcribe_audio",
                    "description": "è½¬å½•éŸ³é¢‘ä¸ºæ–‡å­—",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "audio_path": {
                                "type": "string",
                                "description": "éŸ³é¢‘æ–‡ä»¶è·¯å¾„"
                            }
                        },
                        "required": ["audio_path"]
                    }
                }
            }
        ]

        # è°ƒç”¨LLM
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=messages,
            tools=tools,
            tool_choice="auto"
        )

        message = response.choices[0].message

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
        if message.tool_calls:
            for tool_call in message.tool_calls:
                function_name = tool_call.function.name
                function_args = json.loads(tool_call.function.arguments)

                # æ‰§è¡Œå·¥å…·
                if function_name in self.tools:
                    result = self.tools[function_name]["function"](**function_args)

                    # æ·»åŠ å·¥å…·ç»“æœ
                    messages.append(message)
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": result
                    })

            # å†æ¬¡è°ƒç”¨LLMç”Ÿæˆæœ€ç»ˆå›å¤
            response = self.client.chat.completions.create(
                model="gpt-4",
                messages=messages
            )

        return response.choices[0].message.content
```

## äº”ã€å®æˆ˜æ¡ˆä¾‹ï¼šå¤šæ¨¡æ€å­¦ä¹ åŠ©æ‰‹

```python
class MultimodalLearningAssistant:
    """å¤šæ¨¡æ€å­¦ä¹ åŠ©æ‰‹"""

    def __init__(self):
        self.agent = UnifiedMultimodalAgent()
        self.client = OpenAI()

    def help_with_homework(
        self,
        question: str,
        image_path: Optional[str] = None
    ) -> str:
        """è¾…å¯¼ä½œä¸š"""
        input_data = MultimodalInput()

        # æ„å»ºæç¤º
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªè€å¿ƒçš„å­¦ä¹ åŠ©æ‰‹ã€‚å­¦ç”Ÿçš„é—®é¢˜ï¼š

        {question}

        è¯·æä¾›ï¼š
        1. æ¸…æ™°çš„è§£é‡Š
        2. åˆ†æ­¥éª¤çš„è§£ç­”
        3. ç›¸å…³çŸ¥è¯†ç‚¹
        4. ç±»ä¼¼ä¾‹é¢˜

        ç”¨é¼“åŠ±å’Œå¯å‘çš„è¯­æ°”ã€‚
        """

        input_data.add_text(prompt)

        # æ·»åŠ å›¾ç‰‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if image_path:
            input_data.add_image(image_path)

        # å¤„ç†
        response = self.agent.process(input_data)
        return response

    def analyze_diagram(self, diagram_path: str) -> Dict:
        """åˆ†æå›¾è¡¨"""
        input_data = MultimodalInput()
        input_data.add_image(diagram_path)
        input_data.add_text("""
        è¯·è¯¦ç»†åˆ†æè¿™å¼ å›¾è¡¨ï¼ŒåŒ…æ‹¬ï¼š
        1. å›¾è¡¨ç±»å‹å’Œæ ‡é¢˜
        2. æ•°æ®è¶‹åŠ¿
        3. å…³é”®å‘ç°
        4. æ•°æ®æ¥æºçš„å¯é æ€§è¯„ä¼°
        """)

        response = self.agent.process(input_data)

        return {
            "analysis": response,
            "chart_type": self._detect_chart_type(diagram_path),
            "confidence": 0.9
        }

    def _detect_chart_type(self, image_path: str) -> str:
        """æ£€æµ‹å›¾è¡¨ç±»å‹"""
        # ä½¿ç”¨è§†è§‰Agent
        prompt = "è¿™å¼ å›¾ç‰‡æ˜¯ä»€ä¹ˆç±»å‹çš„å›¾è¡¨ï¼Ÿï¼ˆæŸ±çŠ¶å›¾ã€æŠ˜çº¿å›¾ã€é¥¼å›¾ç­‰ï¼‰"
        return self.agent.vision_agent.answer_question(image_path, prompt)

    def voice_tutoring(self, audio_question_path: str):
        """è¯­éŸ³è¾…å¯¼"""
        # 1. è½¬å½•é—®é¢˜
        question = self.agent.speech_agent.transcribe(audio_question_path)
        print(f"é—®é¢˜ï¼š{question}")

        # 2. ç”Ÿæˆå›ç­”
        response = self.help_with_homework(question)
        print(f"å›ç­”ï¼š{response}")

        # 3. è¯­éŸ³è¾“å‡º
        print("ğŸ”Š æ­£åœ¨ç”Ÿæˆè¯­éŸ³è®²è§£...")
        self.agent.tts_agent.speak(response, voice="nova")

        return response
```

## å…­ã€æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **å¤šæ¨¡æ€æ¶æ„**ï¼šç¼–ç å™¨-èåˆ-è§£ç å™¨çš„ç»Ÿä¸€æ¡†æ¶
2. **è§†è§‰èƒ½åŠ›**ï¼šå›¾åƒæè¿°ã€OCRã€å›¾è¡¨åˆ†æã€è§†è§‰é—®ç­”
3. **è¯­éŸ³èƒ½åŠ›**ï¼šASRï¼ˆè¯†åˆ«ï¼‰ã€TTSï¼ˆåˆæˆï¼‰ã€è¯­éŸ³å¯¹è¯
4. **æ¨¡æ€èåˆ**ï¼šä¸åŒæ¨¡æ€ä¿¡æ¯çš„å¯¹é½ä¸æ•´åˆ
5. **å®é™…åº”ç”¨**ï¼šå­¦ä¹ åŠ©æ‰‹ã€æ— éšœç¢è®¿é—®ã€å†…å®¹å®¡æ ¸ç­‰

### æœ€ä½³å®è·µ

- âœ… **æ¨¡æ€å¯¹é½**ï¼šç¡®ä¿ä¸åŒæ¨¡æ€ä¿¡æ¯çš„ä¸€è‡´æ€§
- âœ… **é”™è¯¯å¤„ç†**ï¼šæ¨¡æ€ç¼ºå¤±æˆ–æŸåæ—¶çš„é™çº§ç­–ç•¥
- âœ… **æ€§èƒ½ä¼˜åŒ–**ï¼šå¤§æ–‡ä»¶å‹ç¼©ã€æ‰¹å¤„ç†
- âœ… **éšç§ä¿æŠ¤**ï¼šç”¨æˆ·ä¸Šä¼ çš„å¤šæ¨¡æ€æ•°æ®å®‰å…¨
- âœ… **æˆæœ¬æ§åˆ¶**ï¼šæ ¹æ®éœ€æ±‚é€‰æ‹©æ¨¡å‹ç²¾åº¦

### å¸¸è§é™·é˜±

- âŒ **æ¨¡æ€å†²çª**ï¼šæ–‡æœ¬å’Œå›¾åƒä¿¡æ¯çŸ›ç›¾
- âŒ **è¿‡åº¦ä¾èµ–è§†è§‰**ï¼šå›¾ç‰‡è´¨é‡å·®å¯¼è‡´è¯¯è¯†åˆ«
- âŒ **å»¶è¿Ÿé—®é¢˜**ï¼šå¤šæ¨¡æ€å¤„ç†è€—æ—¶é•¿
- âŒ **æˆæœ¬çˆ†ç‚¸**ï¼šé«˜æ¸…å›¾ç‰‡å’Œé•¿éŸ³é¢‘æˆæœ¬é«˜

---

## æ¨èé˜…è¯»

- [OpenAI Vision API](https://platform.openai.com/docs/guides/vision)
- [Whisper: Robust Speech Recognition](https://github.com/openai/whisper)
- [CLIP: Connecting Text and Images](https://openai.com/research/clip)

## å…³äºæœ¬ç³»åˆ—

è¿™æ˜¯ã€ŠAI Agentç³»åˆ—æ•™ç¨‹ã€‹çš„ç¬¬7ç¯‡ï¼Œå…±12ç¯‡ã€‚

**ä¸Šä¸€ç¯‡å›é¡¾**ï¼šã€Šè§„åˆ’ä¸æ¨ç†ï¼šAgentå¦‚ä½•åˆ†è§£å¤æ‚ä»»åŠ¡ã€‹

**ä¸‹ä¸€ç¯‡é¢„å‘Š**ï¼šã€ŠAgentè¯„ä¼°ä¸ä¼˜åŒ–ï¼šå¦‚ä½•è¡¡é‡Agentæ€§èƒ½ã€‹

---

*å¦‚æœè¿™ç¯‡æ–‡ç« å¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿ç‚¹èµã€æ”¶è—å’Œåˆ†äº«ï¼æœ‰ä»»ä½•é—®é¢˜æ¬¢è¿åœ¨è¯„è®ºåŒºè®¨è®ºã€‚*

---

**ä¸Šä¸€ç¯‡**ï¼š[è§„åˆ’ä¸æ¨ç†ï¼šAgentå¦‚ä½•åˆ†è§£å¤æ‚ä»»åŠ¡](./article-07-planning-reasoning.md)
**ä¸‹ä¸€ç¯‡**ï¼š[Agentè¯„ä¼°ä¸ä¼˜åŒ–ï¼šå¦‚ä½•è¡¡é‡Agentæ€§èƒ½](./article-10-agent-evaluation.md)

---

**ç³»åˆ—è¯´æ˜**ï¼š
- æœ¬ç³»åˆ—æ–‡ç« æ­£åœ¨æŒç»­æ›´æ–°ä¸­ï¼Œæ¬¢è¿å…³æ³¨ï¼
- æ‰€æœ‰ä»£ç ç¤ºä¾‹å°†åœ¨GitHubä»“åº“å¼€æºï¼š`ai-agent-tutorial-series`
- æœ‰é—®é¢˜æ¬¢è¿åœ¨è¯„è®ºåŒºè®¨è®ºï¼Œæˆ‘ä¼šåŠæ—¶å›å¤
